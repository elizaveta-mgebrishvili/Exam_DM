{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares with Real Data\n",
    "\n",
    "Since we already have practice working with sparse data, we can apply the same techniques to our Amazon dataset. The trick is that because the data is so large, it will be trickier to use our debugging techniques from the <a href=\"1-05_als.ipynb\"></a>previous notebook.\n",
    "\n",
    "## Objectives\n",
    "This notebook demonstrates:\n",
    "* How to build a collaborative filter for large datasets\n",
    "    * [1. Splitting into Train and Test](#1.-Splitting-into-Train-and-Test)\n",
    "    * [2. Alternating Least Squares](#2.-Alternating-Least-Squares)\n",
    "    * [3. Wrap Up](#3.-Wrap-Up)\n",
    "\n",
    "## 1. Splitting into Train and Test    \n",
    "We already did data cleansing in an earlier notebook, so let's load it up along with our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEVNTIQFU2TQ6</td>\n",
       "      <td>B00004ZCB3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1XI6NT41B6E6X</td>\n",
       "      <td>B00004ZCB3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2PMIMM3U3THM7</td>\n",
       "      <td>B00004ZCB3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1E9NFMLMVY61H</td>\n",
       "      <td>B00004ZCB4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXABTEYS7A4A8</td>\n",
       "      <td>B00004ZCB4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  valid\n",
       "0   AEVNTIQFU2TQ6  B00004ZCB3      3.0  False\n",
       "1  A1XI6NT41B6E6X  B00004ZCB3      5.0  False\n",
       "2  A2PMIMM3U3THM7  B00004ZCB3      5.0  False\n",
       "3  A1E9NFMLMVY61H  B00004ZCB4      5.0  False\n",
       "4   AXABTEYS7A4A8  B00004ZCB4      5.0  False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import cupyx.scipy.sparse\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "ratings = cudf.read_csv(\"data/ratings.csv\")\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we split into train and test, let's [factorize](https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.Series.factorize.html) our users and items. Our test set won't be effective if it doesn't use the same indexes as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexes, user_mapping = ratings[\"reviewerID\"].factorize()\n",
    "item_indexes, item_mapping = ratings[\"asin\"].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're building our sparse matrix, let's get a sense of our data along the way. First, let's see how many users there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_count = user_mapping.count()\n",
    "\n",
    "user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost two hundred thousand! Okay, how about the items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_count = item_mapping.count()\n",
    "\n",
    "item_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much bigger than our toy problem with four users and five items. Using [todense](https://docs-cupy.chainer.org/en/stable/reference/generated/cupyx.scipy.sparse.coo_matrix.html#cupyx.scipy.sparse.coo_matrix.todense) will result in memory limitations, which in the real world is both frustrating and expensive.\n",
    "\n",
    "Instead, we'll need to rely on good testing. Let's break our ratings data down into train and test. Please fill in the `FIXME`s below, using the <a href=\"1-05_als_intro.ipynb\">previous lab</a> as a hint if needed. There is one `FIXME` for each input of the `get_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_indexes = ~ratings[\"valid\"]\n",
    "valid_indexes = ratings[\"valid\"]\n",
    "overall = ratings[\"overall\"]\n",
    "\n",
    "def get_dataset(data_selector, user_indexes, item_indexes, overall):\n",
    "    # FIXME: find als rows and columns based on indexes\n",
    "    row = cp.asarray(user_indexes[data_selector])\n",
    "    column = cp.asarray(item_indexes[data_selector])\n",
    "    data = cp.asarray(overall[data_selector])\n",
    "    \n",
    "    # FIXME: build sparse matrix with correct shape\n",
    "    sparse_data = cupyx.scipy.sparse.coo_matrix((data, (row, column)))#, shape=FIXME_E\n",
    "    mask = cp.asarray([1 for _ in range(len(data))], dtype=np.float32)\n",
    "    sparse_mask = cupyx.scipy.sparse.coo_matrix((mask, (row, column)))#, shape=FIXME_E\n",
    "    return row, column, data, sparse_data, sparse_mask\n",
    "\n",
    "train_row, train_column, train_data, train_sparse, train_mask = get_dataset(\n",
    "    train_indexes, user_indexes, item_indexes, overall)\n",
    "\n",
    "valid_row, valid_column, valid_data, valid_sparse, valid_mask = get_dataset(\n",
    "    valid_indexes, user_indexes, item_indexes, overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternating Least Squares\n",
    "\n",
    "It's time to initialize our embeddings. Let's copy over our functions from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = 2\n",
    "shape = (user_count, item_count)\n",
    "\n",
    "def initalize_features(length, embeddings):\n",
    "    return cp.random.rand(embeddings, length) * 2 - 1\n",
    "\n",
    "user_features = initalize_features(shape[0], embeddings)\n",
    "item_features = initalize_features(shape[1], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(user_features, item_features, data, row, column):\n",
    "    predictions = (user_features[:, row] * item_features[:, column]).sum(axis=0)\n",
    "    mean_squared_error = ((predictions - data) ** 2).mean() ** 0.5\n",
    "    \n",
    "    return predictions, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ALS function below is incomplete. Please fill in the `FIXME`s to get it up and running. There is one `FIXME` for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als(values, mask, features, scale=0.01):\n",
    "    # FIXME: add inputs to als algorithm\n",
    "    numerator = values.dot(features.T)\n",
    "    squared_features = (features ** 2).sum(axis=0)\n",
    "    denominator = scale + mask.dot(squared_features)\n",
    "\n",
    "    return (numerator / denominator[:, None]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How low can you go? Run the below cell multiple times to watch the RMSE fall. If the above `FIXME`s are correctly implemented, the RMSE can reach below `1.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.747963577712049\n",
      "RMSE: 4.794625251557147\n",
      "RMSE: 4.495824321072233\n",
      "RMSE: 2.827020752844117\n",
      "RMSE: 1.6624145000709905\n",
      "RMSE: 1.4794606772838934\n",
      "RMSE: 1.4272505019505992\n",
      "RMSE: 1.3626122974733512\n",
      "RMSE: 1.3247180072470262\n",
      "RMSE: 1.302490290161209\n",
      "RMSE: 1.2818167872031527\n",
      "RMSE: 1.2559054958036335\n",
      "RMSE: 1.2298289459765572\n",
      "RMSE: 1.2144743456656533\n",
      "RMSE: 1.2113453010471478\n",
      "RMSE: 1.211979575856461\n",
      "RMSE: 1.2110792515861635\n",
      "RMSE: 1.208835062752098\n",
      "RMSE: 1.2065260535843432\n",
      "RMSE: 1.2049225916329336\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    user_features = als(train_sparse, train_mask, item_features)\n",
    "    item_features = als(train_sparse.T, train_mask.T, user_features)\n",
    "    predictions, error = rmse(\n",
    "        user_features, item_features, valid_data, valid_row, valid_column)\n",
    "\n",
    "    print (\"RMSE:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a human eyeball test to verify our prediction error. Do the predictions follow the same ranking order as the true ratings do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., 3., 5., 4., 4., 5., 5., 5.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.06642584, 4.97837196, 5.16229319, 3.82990864, 4.35054339,\n",
       "       2.62827754, 4.97384671, 5.03891118, 5.2700046 , 5.19224769])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrap Up\n",
    "\n",
    "To save our work, let's group the notebooks in a zip file by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: 1-01_intro.ipynb (deflated 70%)\n",
      "  adding: 1-03_content_based_intro.ipynb (deflated 88%)\n",
      "  adding: 1-05_als_intro.ipynb (deflated 47%)\n",
      "  adding: 1-02_environment.ipynb (deflated 69%)\n",
      "  adding: 1-06_als_real_data.ipynb (deflated 71%)\n",
      "  adding: 1-04_content_based_real_data.ipynb (deflated 87%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r lab1_work.zip . -i '1*.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the zip file, right click on it from the file menu to the left and select \"Download\". The file menu may need to be refreshed by clicking the arrow circle above the list.\n",
    "\n",
    "Congratulations on finishing this set of notebooks! We learned how to do two major types of Recommender Systems: Content-based and Collaborative. While we were able to achieve a lower RMSE with Collaborative Filtering, there are some situations were Content-based wins out. Have an idea what they are? Head back over to the notebook launcher to take a quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
